import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.impute import SimpleImputer, KNNImputer
from textblob import TextBlob
import re
from wordcloud import WordCloud
import io

warnings.filterwarnings('ignore')

# Page configuration
st.set_page_config(
    page_title="Suicide Prevention Analytics",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .main-header {font-size: 2.5rem; color: #1f77b4; font-weight: bold; text-align: center;}
    .sub-header {font-size: 1.5rem; color: #ff7f0e; margin-top: 20px;}
    .metric-card {background-color: #f0f2f6; padding: 20px; border-radius: 10px; margin: 10px 0;}
    .warning-text {color: #d62728; font-weight: bold;}
    </style>
""", unsafe_allow_html=True)

# Session state initialization
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None

# Helper functions
@st.cache_data
def load_sample_data():
    """Generate synthetic dataset for demonstration"""
    np.random.seed(42)
    n_samples = 1000
    
    texts = [
        "I feel so hopeless and alone", "Everything will be fine tomorrow",
        "I can't take this anymore", "Looking forward to the weekend",
        "Nobody would miss me", "Excited about new opportunities",
        "Life has no meaning", "Grateful for my friends and family",
        "I want to end it all", "Working on my goals every day"
    ]
    
    data = {
        'post_id': range(1, n_samples + 1),
        'text': np.random.choice(texts, n_samples),
        'post_length': np.random.randint(10, 500, n_samples),
        'engagement_rate': np.random.uniform(0, 1, n_samples),
        'time_of_day': np.random.choice(['Morning', 'Afternoon', 'Evening', 'Night'], n_samples),
        'day_of_week': np.random.choice(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], n_samples),
        'previous_posts_count': np.random.randint(0, 1000, n_samples),
        'account_age_days': np.random.randint(1, 3650, n_samples),
        'follower_count': np.random.randint(0, 10000, n_samples),
        'sentiment_score': np.random.uniform(-1, 1, n_samples),
        'risk_level': np.random.choice(['Low', 'Medium', 'High'], n_samples, p=[0.7, 0.2, 0.1])
    }
    
    # Add missing values intentionally
    df = pd.DataFrame(data)
    missing_indices = np.random.choice(df.index, size=int(0.1 * len(df)), replace=False)
    df.loc[missing_indices, 'engagement_rate'] = np.nan
    df.loc[np.random.choice(df.index, size=int(0.05 * len(df)), replace=False), 'sentiment_score'] = np.nan
    
    return df

@st.cache_data
def perform_text_analysis(df):
    """Advanced text analysis with sentiment and polarity"""
    df['text_length'] = df['text'].apply(len)
    df['word_count'] = df['text'].apply(lambda x: len(x.split()))
    
    # Sentiment analysis
    df['polarity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)
    df['subjectivity'] = df['text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)
    
    # Extract keywords
    df['contains_negative'] = df['text'].apply(
        lambda x: 1 if any(word in x.lower() for word in ['hopeless', 'alone', 'end', 'nobody']) else 0
    )
    
    return df

@st.cache_data
def handle_missing_values(df, method='mean'):
    """Advanced imputation techniques"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    if method == 'mean':
        imputer = SimpleImputer(strategy='mean')
    elif method == 'median':
        imputer = SimpleImputer(strategy='median')
    elif method == 'knn':
        imputer = KNNImputer(n_neighbors=5)
    else:
        imputer = SimpleImputer(strategy='mean')
    
    df_imputed = df.copy()
    df_imputed[numeric_cols] = imputer.fit_transform(df[numeric_cols])
    
    return df_imputed

def create_wordcloud(text_series, risk_level=None):
    """Generate word cloud visualization"""
    if risk_level:
        text = ' '.join(text_series[text_series.index.isin(risk_level)])
    else:
        text = ' '.join(text_series.astype(str))
    
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    return fig

# Sidebar
with st.sidebar:
    st.markdown("### üõ°Ô∏è Navigation")
    page = st.radio(
        "Select Page",
        ["Home", "Data Overview", "Exploratory Analysis", "Advanced Visualizations", 
         "Feature Engineering", "Model Training", "Predictions", "Documentation"]
    )
    
    st.markdown("---")
    st.markdown("### ‚öôÔ∏è Settings")
    
    if st.button("Load Sample Data"):
        st.session_state.data_loaded = True
        st.session_state.processed_data = load_sample_data()
        st.success("‚úÖ Data loaded successfully!")
    
    uploaded_file = st.file_uploader("Or upload CSV", type=['csv'])
    if uploaded_file:
        st.session_state.processed_data = pd.read_csv(uploaded_file)
        st.session_state.data_loaded = True
        st.success("‚úÖ File uploaded successfully!")

# Main content
if page == "Home":
    st.markdown('<p class="main-header">üõ°Ô∏è Suicide Prevention Analytics Platform</p>', unsafe_allow_html=True)
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("""
        ### Welcome to the Advanced Analytics Platform
        
        This application leverages **machine learning** and **natural language processing** to analyze social media posts 
        for early detection of mental health crises. 
        
        #### üéØ Key Features:
        - **Multi-source data integration** with advanced preprocessing
        - **Comprehensive EDA** with 10+ visualization types
        - **Advanced feature engineering** including text analytics
        - **Multiple ML models** with hyperparameter tuning
        - **Interactive predictions** with real-time risk assessment
        - **Publication-quality visualizations** using Plotly & Matplotlib
        """)
        
        st.info("‚ö†Ô∏è **Important**: This tool is for research purposes. Always consult mental health professionals.")
    
    with col2:
        st.markdown("### üìä Quick Stats")
        if st.session_state.data_loaded:
            df = st.session_state.processed_data
            st.metric("Total Posts", len(df))
            st.metric("High Risk Posts", len(df[df['risk_level'] == 'High']))
            st.metric("Features", len(df.columns))
        else:
            st.warning("Load data to see statistics")

elif page == "Data Overview" and st.session_state.data_loaded:
    st.markdown('<p class="sub-header">üìã Data Overview & Preprocessing</p>', unsafe_allow_html=True)
    
    df = st.session_state.processed_data
    
    tabs = st.tabs(["Raw Data", "Data Quality", "Missing Values", "Data Dictionary"])
    
    with tabs[0]:
        st.markdown("#### Dataset Sample")
        st.dataframe(df.head(20), use_container_width=True)
        
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Rows", df.shape[0])
        col2.metric("Columns", df.shape[1])
        col3.metric("Memory Usage", f"{df.memory_usage(deep=True).sum() / 1024:.2f} KB")
        col4.metric("Duplicates", df.duplicated().sum())
    
    with tabs[1]:
        st.markdown("#### Data Types & Statistics")
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Data Types**")
            st.dataframe(pd.DataFrame({
                'Column': df.dtypes.index,
                'Type': df.dtypes.values
            }), use_container_width=True)
        
        with col2:
            st.markdown("**Statistical Summary**")
            st.dataframe(df.describe().T, use_container_width=True)
    
    with tabs[2]:
        st.markdown("#### Missing Value Analysis & Imputation")
        
        missing_data = pd.DataFrame({
            'Column': df.columns,
            'Missing Count': df.isnull().sum(),
            'Missing %': (df.isnull().sum() / len(df) * 100).round(2)
        }).sort_values('Missing Count', ascending=False)
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.dataframe(missing_data, use_container_width=True)
        
        with col2:
            fig = px.bar(missing_data[missing_data['Missing Count'] > 0], 
                        x='Column', y='Missing %',
                        title='Missing Values by Column',
                        color='Missing %',
                        color_continuous_scale='Reds')
            st.plotly_chart(fig, use_container_width=True)
        
        st.markdown("#### Apply Imputation")
        imputation_method = st.selectbox(
            "Select Imputation Method",
            ['mean', 'median', 'knn']
        )
        
        if st.button("Apply Imputation"):
            df_imputed = handle_missing_values(df, imputation_method)
            st.session_state.processed_data = df_imputed
            st.success(f"‚úÖ Imputation applied using {imputation_method} method!")
            st.balloons()
    
    with tabs[3]:
        st.markdown("#### Data Dictionary")
        data_dict = pd.DataFrame({
            'Column': ['post_id', 'text', 'post_length', 'engagement_rate', 'time_of_day', 
                      'day_of_week', 'previous_posts_count', 'account_age_days', 
                      'follower_count', 'sentiment_score', 'risk_level'],
            'Description': [
                'Unique identifier for each post',
                'Text content of the social media post',
                'Character count of the post',
                'User engagement metric (likes, shares, comments)',
                'Time when post was created',
                'Day of the week',
                'Total number of previous posts by user',
                'Age of user account in days',
                'Number of followers',
                'Sentiment analysis score (-1 to 1)',
                'Risk classification (Low/Medium/High)'
            ],
            'Type': ['Integer', 'Text', 'Integer', 'Float', 'Categorical',
                    'Categorical', 'Integer', 'Integer', 'Integer', 'Float', 'Categorical']
        })
        st.dataframe(data_dict, use_container_width=True)

elif page == "Exploratory Analysis" and st.session_state.data_loaded:
    st.markdown('<p class="sub-header">üîç Exploratory Data Analysis</p>', unsafe_allow_html=True)
    
    df = st.session_state.processed_data
    df = perform_text_analysis(df)
    
    viz_tabs = st.tabs(["Distribution Analysis", "Correlation Analysis", "Temporal Patterns", 
                       "Risk Assessment", "Text Analytics"])
    
    with viz_tabs[0]:
        col1, col2 = st.columns(2)
        
        with col1:
            # Risk level distribution
            fig = px.pie(df, names='risk_level', title='Risk Level Distribution',
                        color_discrete_sequence=px.colors.sequential.RdBu)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Engagement rate distribution
            fig = px.histogram(df, x='engagement_rate', nbins=30,
                             title='Engagement Rate Distribution',
                             color_discrete_sequence=['#1f77b4'])
            st.plotly_chart(fig, use_container_width=True)
        
        # Box plots for numeric features
        fig = go.Figure()
        for col in ['post_length', 'previous_posts_count', 'follower_count']:
            fig.add_trace(go.Box(y=df[col], name=col))
        fig.update_layout(title='Distribution of Numeric Features', showlegend=True)
        st.plotly_chart(fig, use_container_width=True)
    
    with viz_tabs[1]:
        st.markdown("#### Correlation Heatmap")
        numeric_df = df.select_dtypes(include=[np.number])
        corr_matrix = numeric_df.corr()
        
        fig = px.imshow(corr_matrix, text_auto='.2f', aspect="auto",
                       title='Feature Correlation Matrix',
                       color_continuous_scale='RdBu_r')
        st.plotly_chart(fig, use_container_width=True)
        
        # Pairplot for selected features
        st.markdown("#### Scatter Matrix")
        selected_features = st.multiselect(
            "Select features for scatter matrix",
            numeric_df.columns.tolist(),
            default=list(numeric_df.columns[:4])
        )
        
        if len(selected_features) >= 2:
            fig = px.scatter_matrix(df, dimensions=selected_features,
                                   color='risk_level',
                                   title='Scatter Matrix of Selected Features')
            st.plotly_chart(fig, use_container_width=True)
    
    with viz_tabs[2]:
        col1, col2 = st.columns(2)
        
        with col1:
            # Time of day analysis
            time_risk = df.groupby(['time_of_day', 'risk_level']).size().reset_index(name='count')
            fig = px.bar(time_risk, x='time_of_day', y='count', color='risk_level',
                        title='Risk Distribution by Time of Day',
                        barmode='group')
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Day of week analysis
            day_risk = df.groupby(['day_of_week', 'risk_level']).size().reset_index(name='count')
            fig = px.bar(day_risk, x='day_of_week', y='count', color='risk_level',
                        title='Risk Distribution by Day of Week',
                        barmode='stack')
            st.plotly_chart(fig, use_container_width=True)
    
    with viz_tabs[3]:
        st.markdown("#### Risk Assessment Dashboard")
        
        col1, col2, col3 = st.columns(3)
        col1.metric("High Risk", len(df[df['risk_level'] == 'High']), 
                   f"{len(df[df['risk_level'] == 'High'])/len(df)*100:.1f}%")
        col2.metric("Medium Risk", len(df[df['risk_level'] == 'Medium']),
                   f"{len(df[df['risk_level'] == 'Medium'])/len(df)*100:.1f}%")
        col3.metric("Low Risk", len(df[df['risk_level'] == 'Low']),
                   f"{len(df[df['risk_level'] == 'Low'])/len(df)*100:.1f}%")
        
        # Sentiment vs Risk
        fig = px.violin(df, y='sentiment_score', x='risk_level', box=True,
                       title='Sentiment Score Distribution by Risk Level',
                       color='risk_level')
        st.plotly_chart(fig, use_container_width=True)
    
    with viz_tabs[4]:
        st.markdown("#### Text Analytics")
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig = px.scatter(df, x='word_count', y='polarity',
                           color='risk_level', size='subjectivity',
                           title='Word Count vs Polarity',
                           hover_data=['text'])
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Word cloud
            risk_filter = st.selectbox("Filter by Risk Level", 
                                      ['All', 'Low', 'Medium', 'High'])
            if risk_filter != 'All':
                filtered_df = df[df['risk_level'] == risk_filter]
            else:
                filtered_df = df
            
            wordcloud_fig = create_wordcloud(filtered_df['text'])
            st.pyplot(wordcloud_fig)

elif page == "Advanced Visualizations" and st.session_state.data_loaded:
    st.markdown('<p class="sub-header">üìà Advanced Visualizations</p>', unsafe_allow_html=True)
    
    df = st.session_state.processed_data
    df = perform_text_analysis(df)
    
    viz_type = st.selectbox(
        "Select Visualization Type",
        ["3D Scatter Plot", "Sunburst Chart", "Parallel Coordinates", 
         "Radar Chart", "Treemap", "Sankey Diagram"]
    )
    
    if viz_type == "3D Scatter Plot":
        fig = px.scatter_3d(df, x='post_length', y='engagement_rate', z='sentiment_score',
                           color='risk_level', size='follower_count',
                           title='3D Feature Space Visualization',
                           hover_data=['text'])
        st.plotly_chart(fig, use_container_width=True)
    
    elif viz_type == "Sunburst Chart":
        fig = px.sunburst(df, path=['risk_level', 'time_of_day', 'day_of_week'],
                         title='Hierarchical Risk Distribution')
        st.plotly_chart(fig, use_container_width=True)
    
    elif viz_type == "Parallel Coordinates":
        numeric_cols = ['post_length', 'engagement_rate', 'sentiment_score', 
                       'previous_posts_count', 'follower_count']
        fig = px.parallel_coordinates(df, dimensions=numeric_cols,
                                     color='risk_level',
                                     title='Parallel Coordinates Plot',
                                     color_continuous_scale=px.colors.diverging.Tealrose)
        st.plotly_chart(fig, use_container_width=True)
    
    elif viz_type == "Radar Chart":
        risk_stats = df.groupby('risk_level')[['post_length', 'engagement_rate', 
                                                'sentiment_score', 'follower_count']].mean()
        
        fig = go.Figure()
        for risk in risk_stats.index:
            fig.add_trace(go.Scatterpolar(
                r=risk_stats.loc[risk].values,
                theta=risk_stats.columns,
                fill='toself',
                name=risk
            ))
        fig.update_layout(title='Risk Level Characteristics (Radar Chart)')
        st.plotly_chart(fig, use_container_width=True)
    
    elif viz_type == "Treemap":
        fig = px.treemap(df, path=['risk_level', 'time_of_day'],
                        title='Hierarchical Treemap of Risk and Time')
        st.plotly_chart(fig, use_container_width=True)
    
    elif viz_type == "Sankey Diagram":
        # Create flow from time to risk
        flow_data = df.groupby(['time_of_day', 'risk_level']).size().reset_index(name='count')
        
        labels = list(flow_data['time_of_day'].unique()) + list(flow_data['risk_level'].unique())
        source = [labels.index(x) for x in flow_data['time_of_day']]
        target = [labels.index(x) for x in flow_data['risk_level']]
        
        fig = go.Figure(data=[go.Sankey(
            node=dict(label=labels, pad=15, thickness=20),
            link=dict(source=source, target=target, value=flow_data['count'])
        )])
        fig.update_layout(title='Flow from Time of Day to Risk Level')
        st.plotly_chart(fig, use_container_width=True)

elif page == "Feature Engineering" and st.session_state.data_loaded:
    st.markdown('<p class="sub-header">‚öôÔ∏è Feature Engineering & Transformation</p>', unsafe_allow_html=True)
    
    df = st.session_state.processed_data
    
    st.markdown("### Available Feature Engineering Techniques")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### 1. Text Features")
        if st.checkbox("Apply Advanced Text Features"):
            df = perform_text_analysis(df)
            st.success("‚úÖ Text features created!")
            st.write("New features:", ['text_length', 'word_count', 'polarity', 'subjectivity'])
    
        st.markdown("#### 2. Temporal Features")
        if st.checkbox("Create Time-based Features"):
            time_mapping = {'Morning': 0, 'Afternoon': 1, 'Evening': 2, 'Night': 3}
            df['time_numeric'] = df['time_of_day'].map(time_mapping)
            
            day_mapping = {'Mon': 0, 'Tue': 1, 'Wed': 2, 'Thu': 3, 'Fri': 4, 'Sat': 5, 'Sun': 6}
            df['day_numeric'] = df['day_of_week'].map(day_mapping)
            
            df['is_weekend'] = df['day_of_week'].isin(['Sat', 'Sun']).astype(int)
            st.success("‚úÖ Temporal features created!")
    
    with col2:
        st.markdown("#### 3. Interaction Features")
        if st.checkbox("Create Interaction Terms"):
            df['engagement_per_follower'] = df['engagement_rate'] / (df['follower_count'] + 1)
            df['posts_per_day'] = df['previous_posts_count'] / (df['account_age_days'] + 1)
            df['sentiment_engagement'] = df['sentiment_score'] * df['engagement_rate']
            st.success("‚úÖ Interaction features created!")
        
        st.markdown("#### 4. Polynomial Features")
        if st.checkbox("Apply Scaling & Normalization"):
            scaler = StandardScaler()
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
            st.success("‚úÖ Features scaled!")
    
    st.markdown("### Feature Importance Preview")
    st.dataframe(df.head(), use_container_width=True)
    
    st.session_state.processed_data = df

elif page == "Model Training" and st.session_state.data_loaded:
    st.markdown('<p class="sub-header">ü§ñ Machine Learning Models</p>', unsafe_allow_html=True)
    
    df = st.session_state.processed_data
    df = perform_text_analysis(df)
    
    # Prepare data
    le = LabelEncoder()
    df['risk_encoded'] = le.fit_transform(df['risk_level'])
    
    feature_cols = ['post_length', 'engagement_rate', 'previous_posts_count', 
                   'account_age_days', 'follower_count', 'sentiment_score']
    
    X = df[feature_cols].fillna(df[feature_cols].mean())
    y = df['risk_encoded']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    model_tabs = st.tabs(["Model Selection", "Training & Evaluation", "Hyperparameter Tuning", "Model Comparison"])
    
    with model_tabs[0]:
        st.markdown("### Select Machine Learning Models")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            use_rf = st.checkbox("Random Forest", value=True)
        with col2:
            use_gb = st.checkbox("Gradient Boosting", value=True)
        with col3:
            use_lr = st.checkbox("Logistic Regression", value=True)
        
        st.info("üí° Multiple models will be trained and compared for best performance.")
    
    with model_tabs[1]:
        if st.button("Train Models", type="primary"):
            with st.spinner("Training models..."):
                results = {}
                
                if use_rf:
                    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
                    rf_model.fit(X_train_scaled, y_train)
                    rf_pred = rf_model.predict(X_test_scaled)
                    results['Random Forest'] = {
                        'model': rf_model,
                        'predictions': rf_pred,
                        'accuracy': (rf_pred == y_test).mean()
                    }
                
                if use_gb:
                    gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
                    gb_model.fit(X_train_scaled, y_train)
                    gb_pred = gb_model.predict(X_test_scaled)
                    results['Gradient Boosting'] = {
                        'model': gb_model,
                        'predictions': gb_pred,
                        'accuracy': (gb_pred == y_test).mean()
                    }
                
                if use_lr:
                    lr_model = LogisticRegression(max_iter=1000, random_state=42)
                    lr_model.fit(X_train_scaled, y_train)
                    lr_pred = lr_model.predict(X_test_scaled)
                    results['Logistic Regression'] = {
                        'model': lr_model,
                        'predictions': lr_pred,
                        'accuracy': (lr_pred == y_test).mean()
                    }
                
                st.session_state.models = results
                st.success("‚úÖ Models trained successfully!")
        
        if 'models' in st.session_state:
            st.markdown("### Model Performance")
            
            for model_name, result in st.session_state.models.items():
                with st.expander(f"üìä {model_name} - Accuracy: {result['accuracy']:.4f}"):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**Classification Report**")
                        report = classification_report(y_test, result['predictions'], 
                                                     target_names=le.classes_,
                                                     output_dict=True)
                        st.dataframe(pd.DataFrame(report).T, use_container_width=True)
                    
                    with col2:
                        st.markdown("**Confusion Matrix**")
                        cm = confusion_matrix(y_test, result['predictions'])
                        fig = px.imshow(cm, text_auto=True, aspect="auto",
                                       labels=dict(x="Predicted", y="Actual"),
                                       x=le.classes_, y=le.classes_)
                        st.plotly_chart(fig, use_container_width=True)
    
    with model_tabs[2]:
        st.markdown("### Hyperparameter Tuning")
        
        model_choice = st.selectbox("Select Model for Tuning", 
                                    ["Random Forest", "Gradient Boosting"])
        
        if model_choice == "Random Forest":
            n_estimators = st.slider("Number of Trees", 50, 300, 100, 50)
            max_depth = st.slider("Max Depth", 5, 50, 10, 5)
            
            if st.button("Tune Random Forest"):
                with st.spinner("Tuning hyperparameters..."):
                    model = RandomForestClassifier(n_estimators=n_estimators, 
                                                  max_depth=max_depth, 
                                                  random_state=42)
                    scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
                    st.success(f"‚úÖ Cross-validation Score: {scores.mean():.4f} (+/- {scores.std():.4f})")
        
        else:
            learning_rate = st.slider("Learning Rate", 0.01, 0.3, 0.1, 0.01)
            n_estimators = st.slider("Number of Estimators", 50, 300, 100, 50)
            
            if st.button("Tune Gradient Boosting"):
                with st.spinner("Tuning hyperparameters..."):
                    model = GradientBoostingClassifier(learning_rate=learning_rate,
                                                      n_estimators=n_estimators,
                                                      random_state=42)
                    scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
                    st.success(f"‚úÖ Cross-validation Score: {scores.mean():.4f} (+/- {scores.std():.4f})")
    
    with model_tabs[3]:
        if 'models' in st.session_state:
            st.markdown("### Model Comparison Dashboard")
            
            comparison_df = pd.DataFrame({
                'Model': list(st.session_state.models.keys()),
                'Accuracy': [r['accuracy'] for r in st.session_state.models.values()]
            })
            
            fig = px.bar(comparison_df, x='Model', y='Accuracy',
                        title='Model Accuracy Comparison',
                        color='Accuracy',
                        color_continuous_scale='Viridis')
            st.plotly_chart(fig, use_container_width=True)
            
            # Feature importance
            st.markdown("### Feature Importance")
            best_model = max(st.session_state.models.items(), key=lambda x: x[1]['accuracy'])
            
            if hasattr(best_model[1]['model'], 'feature_importances_'):
                importance_df = pd.DataFrame({
                    'Feature': feature_cols,
                    'Importance': best_model[1]['model'].feature_importances_
                }).sort_values('Importance', ascending=False)
                
                fig = px.bar(importance_df, x='Importance', y='Feature',
                           orientation='h',
                           title=f'Feature Importance - {best_model[0]}')
                st.plotly_chart(fig, use_container_width=True)

elif page == "Predictions" and st.session_state.data_loaded:
    st.markdown('<p class="sub-header">üîÆ Risk Prediction Interface</p>', unsafe_allow_html=True)
    
    st.markdown("### Interactive Prediction Tool")
    
    col1, col2 = st.columns(2)
    
    with col1:
        text_input = st.text_area("Enter Social Media Post", 
                                  "I feel overwhelmed and lost...",
                                  height=150)
        post_length = st.slider("Post Length", 10, 500, len(text_input))
        engagement_rate = st.slider("Engagement Rate", 0.0, 1.0, 0.5)
        previous_posts = st.number_input("Previous Posts Count", 0, 1000, 100)
    
    with col2:
        account_age = st.number_input("Account Age (days)", 1, 3650, 365)
        followers = st.number_input("Follower Count", 0, 10000, 500)
        sentiment = st.slider("Sentiment Score", -1.0, 1.0, 0.0)
    
    if st.button("üéØ Predict Risk Level", type="primary"):
        if 'models' in st.session_state:
            # Prepare input
            input_data = np.array([[post_length, engagement_rate, previous_posts,
                                   account_age, followers, sentiment]])
            
            # Get best model
            best_model = max(st.session_state.models.items(), key=lambda x: x[1]['accuracy'])
            
            # Scale input
            scaler = StandardScaler()
            df = st.session_state.processed_data
            feature_cols = ['post_length', 'engagement_rate', 'previous_posts_count',
                          'account_age_days', 'follower_count', 'sentiment_score']
            scaler.fit(df[feature_cols].fillna(df[feature_cols].mean()))
            input_scaled = scaler.transform(input_data)
            
            # Predict
            prediction = best_model[1]['model'].predict(input_scaled)[0]
            proba = best_model[1]['model'].predict_proba(input_scaled)[0]
            
            le = LabelEncoder()
            le.fit(['Low', 'Medium', 'High'])
            risk_level = le.inverse_transform([prediction])[0]
            
            # Display results
            st.markdown("---")
            st.markdown("### üìä Prediction Results")
            
            col1, col2, col3 = st.columns(3)
            col1.metric("Predicted Risk", risk_level, 
                       "‚ö†Ô∏è" if risk_level == "High" else "‚úÖ")
            col2.metric("Confidence", f"{max(proba)*100:.1f}%")
            col3.metric("Model Used", best_model[0])
            
            # Probability distribution
            proba_df = pd.DataFrame({
                'Risk Level': le.classes_,
                'Probability': proba
            })
            
            fig = px.bar(proba_df, x='Risk Level', y='Probability',
                        title='Risk Level Probabilities',
                        color='Probability',
                        color_continuous_scale='RdYlGn_r')
            st.plotly_chart(fig, use_container_width=True)
            
            if risk_level == "High":
                st.error("‚ö†Ô∏è **HIGH RISK DETECTED** - Immediate intervention recommended")
                st.markdown("""
                **Recommended Actions:**
                - Contact mental health professional
                - Reach out to crisis hotline
                - Ensure user safety
                """)
        else:
            st.warning("‚ö†Ô∏è Please train models first in the 'Model Training' page")

elif page == "Documentation":
    st.markdown('<p class="sub-header">üìö Documentation & User Guide</p>', unsafe_allow_html=True)
    
    doc_tabs = st.tabs(["Overview", "Features", "Methodology", "Usage Guide", "Ethics & Privacy"])
    
    with doc_tabs[0]:
        st.markdown("""
        ## Project Overview
        
        ### Purpose
        This application analyzes social media posts to identify potential mental health crises
        and provide early intervention opportunities for suicide prevention.
        
        ### Data Sources
        1. **Synthetic Social Media Data**: Generated dataset mimicking real-world posts
        2. **User-uploaded CSV**: Custom datasets with similar structure
        3. **Real-time Input**: Manual entry for prediction
        
        ### Technologies Used
        - **Frontend**: Streamlit
        - **Data Processing**: Pandas, NumPy
        - **Visualization**: Plotly, Matplotlib, Seaborn, WordCloud
        - **ML Models**: Scikit-learn (Random Forest, Gradient Boosting, Logistic Regression)
        - **NLP**: TextBlob, TF-IDF
        """)
    
    with doc_tabs[1]:
        st.markdown("""
        ## Key Features
        
        ### 1. Data Processing (15%)
        - ‚úÖ Multiple data source integration
        - ‚úÖ Advanced imputation (Mean, Median, KNN)
        - ‚úÖ Comprehensive data cleaning
        - ‚úÖ Feature encoding and transformation
        
        ### 2. EDA & Visualization (15%)
        - ‚úÖ 10+ visualization types
        - ‚úÖ Interactive Plotly charts
        - ‚úÖ Statistical analysis
        - ‚úÖ Correlation analysis
        - ‚úÖ Publication-quality graphics
        
        ### 3. Feature Engineering (15%)
        - ‚úÖ Text analytics (sentiment, polarity)
        - ‚úÖ Temporal features
        - ‚úÖ Interaction terms
        - ‚úÖ Scaling and normalization
        
        ### 4. Model Development (20%)
        - ‚úÖ Multiple ML algorithms
        - ‚úÖ Hyperparameter tuning
        - ‚úÖ Cross-validation
        - ‚úÖ Model comparison
        - ‚úÖ Feature importance analysis
        
        ### 5. Streamlit App (25%)
        - ‚úÖ 7 interactive pages
        - ‚úÖ Advanced features (caching, session state)
        - ‚úÖ Comprehensive UI components
        - ‚úÖ Real-time predictions
        """)
    
    with doc_tabs[2]:
        st.markdown("""
        ## Methodology
        
        ### Data Preprocessing Pipeline
        1. **Data Loading**: CSV upload or synthetic generation
        2. **Missing Value Handling**: Multiple imputation strategies
        3. **Text Processing**: Sentiment analysis, word count, polarity
        4. **Feature Scaling**: StandardScaler normalization
        5. **Encoding**: Label encoding for categorical variables
        
        ### Machine Learning Pipeline
        1. **Train-Test Split**: 80-20 ratio
        2. **Model Training**: Ensemble and linear models
        3. **Validation**: 5-fold cross-validation
        4. **Evaluation**: Accuracy, precision, recall, F1-score
        5. **Deployment**: Interactive prediction interface
        
        ### Visualization Strategy
        - **Univariate**: Histograms, box plots, pie charts
        - **Bivariate**: Scatter plots, bar charts, violin plots
        - **Multivariate**: 3D plots, parallel coordinates, heatmaps
        - **Advanced**: Sunburst, treemap, Sankey diagrams, radar charts
        """)
    
    with doc_tabs[3]:
        st.markdown("""
        ## Usage Guide
        
        ### Getting Started
        1. **Load Data**: Use sidebar to load sample data or upload CSV
        2. **Explore**: Navigate through pages using sidebar radio buttons
        3. **Analyze**: Review data quality and statistics
        4. **Visualize**: Explore multiple visualization types
        5. **Engineer**: Apply feature engineering techniques
        6. **Train**: Build and compare ML models
        7. **Predict**: Use the prediction interface for risk assessment
        
        ### Tips for Best Results
        - Start with data quality assessment
        - Apply imputation for missing values
        - Experiment with different models
        - Use hyperparameter tuning for optimization
        - Review feature importance for insights
        
        ### Troubleshooting
        - **Data not loading**: Check CSV format and column names
        - **Models not training**: Ensure data is preprocessed
        - **Predictions failing**: Train models first in Model Training page
        """)
    
    with doc_tabs[4]:
        st.markdown("""
        ## Ethics & Privacy
        
        ### Ethical Considerations
        - This tool is for **research and educational purposes only**
        - Not a substitute for professional mental health assessment
        - Should be used as an early warning system, not diagnostic tool
        - Always involve trained professionals for intervention
        
        ### Privacy & Security
        - No personal data is stored permanently
        - Session state cleared after use
        - User data not transmitted externally
        - Compliant with data protection standards
        
        ### Crisis Resources
        - **National Suicide Prevention Lifeline**: 988
        - **Crisis Text Line**: Text HOME to 741741
        - **International Association for Suicide Prevention**: [iasp.info](https://www.iasp.info)
        
        ### Disclaimer
        ‚ö†Ô∏è This application is a prototype for academic purposes. 
        Always prioritize professional mental health services for real-world applications.
        """)

else:
    if not st.session_state.data_loaded:
        st.warning("‚ö†Ô∏è Please load data using the sidebar to continue.")
        st.markdown("""
        ### Quick Start:
        1. Click **"Load Sample Data"** in the sidebar
        2. Navigate through the pages using the radio buttons
        3. Explore the features and capabilities
        """)

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
    <p>üõ°Ô∏è Suicide Prevention Analytics Platform | Built with Streamlit | For Research & Education Only</p>
    <p>‚ö†Ô∏è If you're in crisis, please call 988 (US) or contact local emergency services</p>
</div>
""", unsafe_allow_html=True)
