import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.impute import SimpleImputer, KNNImputer
from textblob import TextBlob
import re
from wordcloud import WordCloud
import time

warnings.filterwarnings('ignore')

# Page config
st.set_page_config(page_title="MindGuard Analytics", layout="wide", initial_sidebar_state="expanded")

# Custom CSS with ocean blue theme
st.markdown("""
<style>
    /* Gradient background animation */
    @keyframes gradientShift {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }
    
    .stApp {
        background: linear-gradient(-45deg, #e0f2f7, #f0f4f8, #e8f5e9, #fff9e6, #fce4ec);
        background-size: 400% 400%;
        animation: gradientShift 20s ease infinite;
        background-attachment: fixed;
    }
    
    /* Vibrant but readable sidebar */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #1976d2 0%, #1565c0 50%, #0d47a1 100%) !important;
        box-shadow: 4px 0 15px rgba(0, 0, 0, 0.1);
    }
    
    [data-testid="stSidebar"] * {
        color: white !important;
    }
    
    /* Headers with DARK text for readability */
    .main-header {
        font-size: 3rem !important;
        font-weight: 900 !important;
        text-align: center !important;
        margin: 30px 0 !important;
        color: #2c3e50 !important;
        text-shadow: 1px 1px 2px rgba(255,255,255,0.5);
    }
    
    /* Buttons with excellent contrast */
    .stButton > button {
        background: linear-gradient(135deg, #1976d2 0%, #1565c0 100%) !important;
        color: white !important;
        border: none !important;
        padding: 12px 28px !important;
        border-radius: 25px !important;
        font-weight: 700 !important;
        font-size: 1rem !important;
        box-shadow: 0 4px 15px rgba(25, 118, 210, 0.3) !important;
        transition: all 0.3s !important;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px) !important;
        box-shadow: 0 6px 20px rgba(25, 118, 210, 0.4) !important;
    }
    
    /* Dark text for metrics */
    [data-testid="stMetricValue"] {
        color: #1a1a1a !important;
        font-weight: 800 !important;
    }
    
    [data-testid="stMetricLabel"] {
        color: #2c3e50 !important;
        font-weight: 600 !important;
    }
    
    /* All text elements dark for readability */
    .stMarkdown, .stText, p, span, div {
        color: #1a1a1a !important;
    }
    
    h1, h2, h3, h4, h5, h6 {
        color: #1a1a1a !important;
        font-weight: 700 !important;
    }
    
    /* Info boxes with dark text */
    .stAlert {
        color: #1a1a1a !important;
    }
    
    /* Tabs */
    .stTabs [data-baseweb="tab-list"] button {
        color: #1a1a1a !important;
        font-weight: 600 !important;
    }
    
    .stTabs [data-baseweb="tab-list"] button[aria-selected="true"] {
        background: linear-gradient(135deg, #1976d2, #1565c0) !important;
        color: white !important;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False

def load_data():
    """Load mental health dataset"""
    np.random.seed(42)
    n = 1000
    
    risk_levels = np.random.choice(['Low', 'Medium', 'High'], n, p=[0.5, 0.3, 0.2])
    
    texts = []
    for risk in risk_levels:
        if risk == 'High':
            texts.append(np.random.choice([
                "I want to end it all", "Nobody would miss me", "I can't do this anymore",
                "Life is not worth living", "I wish I was dead", "Everyone would be better off without me"
            ]))
        elif risk == 'Medium':
            texts.append(np.random.choice([
                "I'm feeling really down", "Nothing seems to matter", "I feel so alone",
                "Everything is falling apart", "I don't know what to do", "Feeling hopeless today"
            ]))
        else:
            texts.append(np.random.choice([
                "Having a good day", "Feeling grateful", "Things are looking up",
                "Spent time with family", "Enjoyed my morning coffee", "Beautiful weather today"
            ]))
    
    data = {
        'text': texts,
        'risk_level': risk_levels,
        'post_length': np.random.randint(20, 200, n),
        'engagement_rate': np.random.uniform(0, 1, n),
        'sentiment_score': np.random.uniform(-1, 1, n),
        'previous_posts_count': np.random.randint(0, 100, n),
        'follower_count': np.random.randint(10, 10000, n),
        'time_of_day': np.random.choice(['Morning', 'Afternoon', 'Evening', 'Night'], n),
        'day_of_week': np.random.choice(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], n),
        'timestamp': pd.date_range('2024-01-01', periods=n, freq='H')
    }
    
    return pd.DataFrame(data)

def perform_text_analysis(df):
    """Add text analysis features"""
    if 'text' not in df.columns:
        return df
    
    df['text_length'] = df['text'].str.len()
    df['word_count'] = df['text'].str.split().str.len()
    
    return df

def handle_missing_values(df, method='mean'):
    """Handle missing values"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df_imputed = df.copy()
    
    if method == 'mean':
        imputer = SimpleImputer(strategy='mean')
    elif method == 'median':
        imputer = SimpleImputer(strategy='median')
    else:
        imputer = KNNImputer(n_neighbors=5)
    
    df_imputed[numeric_cols] = imputer.fit_transform(df[numeric_cols])
    return df_imputed

def create_wordcloud(text_series):
    """Generate word cloud"""
    text = ' '.join(text_series.astype(str))
    wordcloud = WordCloud(width=800, height=400, background_color='white',
                         colormap='viridis').generate(text)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    return fig

def analyze_mental_health_risk_transformer(text):
    """
    New and improved suicide-risk classifier using:
    - Sentence-BERT embeddings
    - Expanded semantic reference clusters
    - Emotion/sentiment signals
    - Unrelated-content detection
    """
    try:
        from sentence_transformers import SentenceTransformer
        from sklearn.metrics.pairwise import cosine_similarity
        from textblob import TextBlob
    except Exception as e:
        st.error("âš ï¸ Missing dependency for transformer analysis. Install sentence-transformers and textblob.")
        return {
            'risk_level': 'Medium',
            'risk_score': 50,
            'confidence': 0.5,
            'probabilities': np.array([0.33, 0.34, 0.33]),
            'detected_factors': ['âŒ sentence-transformers not available'],
            'sentiment': 0,
            'is_safe_unrelated': False
        }

    # Load model (small, fast, good semantic performance)
    model = SentenceTransformer("all-MiniLM-L6-v2")

    text_clean = text.strip()
    if not text_clean:
        return {
            'risk_level': 'Low',
            'risk_score': 1,
            'confidence': 0.95,
            'probabilities': np.array([0.95, 0.04, 0.01]),
            'detected_factors': ['ğŸŸ¢ Empty or whitespace-only input â€” treated as no risk'],
            'sentiment': 0.0,
            'is_safe_unrelated': True
        }

    # Sentiment
    blob = TextBlob(text)
    sentiment = blob.sentiment.polarity

    # Input embedding
    emb = model.encode([text_clean])[0]

    # Reference clusters (expanded and diverse)
    high_risk_examples = [
        "I want to die", "I don't want to live", "life is not worth living",
        "I want to end everything", "I feel like killing myself",
        "I can't continue anymore", "I feel completely hopeless",
        "I want to disappear forever", "there is no reason to live",
        "my life is useless", "I wish I was dead", "I have thought about ending my life"
    ]

    medium_risk_examples = [
        "I am really struggling", "I feel lost and empty",
        "I hate my life sometimes", "I feel sad and hopeless",
        "nothing makes sense anymore", "I donâ€™t feel okay",
        "I feel like giving up", "everything is falling apart",
        "I canâ€™t handle this stress", "I donâ€™t feel like myself",
        "sometimes I think about whether to continue"
    ]

    low_risk_examples = [
        "I am fine", "I love my life", "I am doing okay",
        "I feel happy today", "I want to live a good life",
        "enjoying my day", "the weather is nice",
        "I like spending time with my friends", "Iâ€™m having a good day",
        "everything is normal", "I have plans for the weekend"
    ]

    unrelated_examples = [
        "I want to eat chocolate", "the car is fast", "I like programming",
        "I went shopping yesterday", "the weather is sunny",
        "I want to study today", "I am learning python",
        "I have an exam tomorrow", "I want to bake a cake", "My cat is cute"
    ]

    # Compute embeddings for references
    high_emb = model.encode(high_risk_examples)
    med_emb = model.encode(medium_risk_examples)
    low_emb = model.encode(low_risk_examples)
    unrelated_emb = model.encode(unrelated_examples)

    # Max similarity to each reference set
    sim_high = float(cosine_similarity([emb], high_emb).max())
    sim_med = float(cosine_similarity([emb], med_emb).max())
    sim_low = float(cosine_similarity([emb], low_emb).max())
    sim_unrelated = float(cosine_similarity([emb], unrelated_emb).max())

    # 1) If the text is semantically closer to unrelated examples and sentiment is not strongly negative => Low
    if sim_unrelated > max(sim_high, sim_med, sim_low) and sentiment >= -0.2:
        return {
            'risk_level': 'Low',
            'risk_score': 3,
            'confidence': 0.96,
            'probabilities': np.array([0.96, 0.03, 0.01]),
            'detected_factors': [
                'ğŸŸ¢ No suicide intent detected.',
                'ğŸŸ¢ Content appears unrelated to mental-health crisis.',
                f'ğŸ” Similarity to unrelated examples: {sim_unrelated:.2f}'
            ],
            'sentiment': sentiment,
            'high_similarity': sim_high,
            'medium_similarity': sim_med,
            'low_similarity': sim_low,
            'is_safe_unrelated': True
        }

    # 2) Very positive sentiment => low risk (defensive rule)
    if sentiment > 0.45:
        return {
            'risk_level': 'Low',
            'risk_score': 5,
            'confidence': 0.9,
            'probabilities': np.array([0.9, 0.08, 0.02]),
            'detected_factors': [
                'ğŸŸ¢ Positive emotional tone detected.',
                f'ğŸ’­ Sentiment: {sentiment:.2f}'
            ],
            'sentiment': sentiment,
            'high_similarity': sim_high,
            'medium_similarity': sim_med,
            'low_similarity': sim_low,
            'is_safe_unrelated': False
        }

    # 3) Combine similarities and sentiment into raw scores
    raw_high = sim_high * 1.45 + (0.15 if sentiment < -0.25 else 0.0)
    raw_med = sim_med * 1.2 + (0.08 if sentiment < -0.15 else 0.0)
    raw_low = sim_low * 1.0 + (0.12 if sentiment > 0.05 else 0.0)

    total = raw_high + raw_med + raw_low + 1e-9
    p_high = raw_high / total
    p_med = raw_med / total
    p_low = raw_low / total

    # Decision thresholds tuned for balanced sensitivity
    if p_high > 0.45 and sim_high > 0.45 and sentiment <= 0.15:
        risk_level = "High"
    elif p_med > 0.40:
        risk_level = "Medium"
    else:
        risk_level = "Low"

    detected = [
        f"ğŸ” Similarity â†’ High: {sim_high:.2f}",
        f"ğŸ” Similarity â†’ Medium: {sim_med:.2f}",
        f"ğŸ” Similarity â†’ Low: {sim_low:.2f}",
        f"ğŸ’­ Sentiment: {sentiment:.2f}"
    ]
    if sim_high > 0.55 and sentiment < -0.2:
        detected.append("ğŸ”´ Strong semantic match to suicidal ideation.")
    elif sim_med > 0.5:
        detected.append("ğŸŸ¡ Moderate semantic match to distress/ideation.")

    confidence = max(p_low, p_med, p_high)

    return {
        'risk_level': risk_level,
        'risk_score': int(p_high * 100 + p_med * 40),
        'confidence': float(confidence),
        'probabilities': np.array([p_low, p_med, p_high]),
        'detected_factors': detected,
        'sentiment': sentiment,
        'high_similarity': sim_high,
        'medium_similarity': sim_med,
        'low_similarity': sim_low,
        'is_safe_unrelated': False
    }

# SIDEBAR
with st.sidebar:
    st.markdown("<h2 style='text-align: center; margin-bottom: 20px;'>ğŸ›¡ï¸ MindGuard</h2>", unsafe_allow_html=True)
    
    page = st.radio("", 
                   ["ğŸ  Home", "ğŸ“Š Data Hub", "ğŸ” Explorer", "ğŸ¨ Visuals Pro", 
                    "âš™ï¸ Engineer", "ğŸ¤– AI Models", "ğŸ”® Predict", "ğŸ“š Docs"],
                   label_visibility="collapsed")
    
    page = page.split(" ", 1)[1]
    
    st.markdown("---")
    st.markdown("### ğŸ†˜ Crisis Resources")
    st.markdown("""
    **Suicide Prevention Lifeline**
    ğŸ“ 988 (US)
    
    **Crisis Text Line**
    ğŸ“± Text HOME to 741741
    
    **International**
    ğŸŒ findahelpline.com
    """)

# HOME PAGE
if page == "Home":
    st.markdown('<p class="main-header">ğŸ›¡ï¸ MINDGUARD ANALYTICS</p>', unsafe_allow_html=True)
    
    st.markdown("""
    <div style='text-align: center; padding: 30px; background: white; border-radius: 15px; box-shadow: 0 4px 15px rgba(0,0,0,0.1); margin: 20px 0;'>
        <h2 style='color: #1a1a1a;'>AI-Powered Mental Health Risk Assessment</h2>
        <p style='font-size: 1.2rem; color: #2c3e50;'>
            Using transformer-based NLP with sentence embeddings for accurate semantic understanding
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div style='background: linear-gradient(135deg, #667eea, #764ba2); padding: 30px; border-radius: 15px; color: white; text-align: center;'>
            <h3>ğŸ§  Transformer-Based</h3>
            <p>Sentence-BERT embeddings for semantic understanding</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div style='background: linear-gradient(135deg, #f093fb, #f5576c); padding: 30px; border-radius: 15px; color: white; text-align: center;'>
            <h3>ğŸ“Š Multi-Class</h3>
            <p>Low, Medium, High risk classification</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div style='background: linear-gradient(135deg, #4facfe, #00f2fe); padding: 30px; border-radius: 15px; color: white; text-align: center;'>
            <h3>ğŸ¯ Contextual</h3>
            <p>Understands meaning, not just keywords</p>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("<br><br>", unsafe_allow_html=True)
    
    if st.button("ğŸ“‚ LOAD DEMO DATA", use_container_width=True, type="primary"):
        with st.spinner("Loading data..."):
            st.session_state.raw_data = load_data()
            st.session_state.processed_data = st.session_state.raw_data.copy()
            st.session_state.data_loaded = True
            time.sleep(1)
        st.success("âœ… Data loaded successfully!")
        st.rerun()


elif page == "Data Hub" and st.session_state.data_loaded:
    st.markdown('<p class="main-header">ğŸ“Š DATA HUB</p>', unsafe_allow_html=True)
    
    df = st.session_state.processed_data
    
    tabs = st.tabs(["ğŸ” Dataset", "ğŸ’ Quality", "ğŸ§¹ Clean", "ğŸ“– Dictionary"])
    
    with tabs[0]:
        st.markdown("### ğŸ“‹ Dataset Overview")
        
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Total Records", len(df))
        col2.metric("Features", len(df.columns))
        col3.metric("High Risk", len(df[df['risk_level']=='High']))
        col4.metric("Low Risk", len(df[df['risk_level']=='Low']))
        
        st.dataframe(df.head(10), use_container_width=True)
        
        st.markdown("### ğŸ“Š Statistical Summary")
        st.dataframe(df.describe(), use_container_width=True)
        st.info("ğŸ“Š **What this table shows:** Statistical summary of numeric features. Mean = average, Std = spread, Min/Max = range, 25%/50%/75% = quartiles showing distribution.")
        
    with tabs[1]:
        st.markdown("### ğŸ’ Data Quality Report")
        
        missing_df = pd.DataFrame({
            'Feature': df.columns,
            'Missing': df.isnull().sum(),
            'Percent': (df.isnull().sum() / len(df) * 100).round(2)
        })
        
        st.dataframe(missing_df, use_container_width=True)
        st.info("â“ **What this table shows:** Count and percentage of missing values per feature. Higher percentages need attention before analysis.")
        
    with tabs[2]:
        st.markdown("### ğŸ§¹ Data Cleaning")
        
        method = st.selectbox("Imputation Method", ["mean", "median", "knn"])
        
        if st.button("Clean Data"):
            st.session_state.processed_data = handle_missing_values(df, method)
            st.success("âœ… Data cleaned!")
    
    with tabs[3]:
        st.markdown("### ğŸ“– Data Dictionary")
        st.markdown("""
        | Feature | Description |
        |---------|-------------|
        | text | Social media post content |
        | risk_level | Assessed risk (Low/Medium/High) |
        | post_length | Character count |
        | engagement_rate | User engagement (0-1) |
        | sentiment_score | Sentiment polarity (-1 to 1) |
        | previous_posts_count | User's post history |
        | follower_count | Number of followers |
        | time_of_day | When posted |
        | day_of_week | Day of posting |
        """)

elif page == "Explorer" and st.session_state.data_loaded:
    st.markdown('<p class="main-header">ğŸ” DATA EXPLORER PRO</p>', unsafe_allow_html=True)
    
    df = st.session_state.processed_data
    df = perform_text_analysis(df)
    
    viz_tabs = st.tabs(["ğŸ“Š Distributions", "ğŸ”— Correlations", "â° Time", "âš ï¸ Risk", "ğŸ“ Text"])
    
    with viz_tabs[0]:
        st.markdown("### ğŸ“Š Distribution Analysis")
        
        col1, col2 = st.columns(2)
        
        with col1:
            fig = px.pie(df, names='risk_level', 
                        title='ğŸ¯ Risk Distribution',
                        color_discrete_map={'Low': '#43e97b', 'Medium': '#f5af19', 'High': '#ff6b6b'},
                        hole=0.5)
            fig.update_traces(textposition='inside', textinfo='percent+label',
                            marker=dict(line=dict(color='white', width=2)))
            fig.update_layout(font=dict(size=13, color='#1a1a1a'))
            st.plotly_chart(fig, use_container_width=True)
            st.info("ğŸ“Š **What this shows:** This pie chart displays the percentage breakdown of all posts by risk level (Low/Medium/High). It helps you quickly identify what proportion of content requires immediate attention versus standard monitoring.")
        
        with col2:
            fig = px.histogram(df, x='engagement_rate', nbins=40,
                             title='ğŸ“ˆ Engagement Distribution',
                             color_discrete_sequence=['#667eea'])
            fig.update_traces(marker=dict(line=dict(color='white', width=1)))
            fig.update_layout(font=dict(size=13, color='#1a1a1a'))
            st.plotly_chart(fig, use_container_width=True)
            st.info("ğŸ“ˆ **What this shows:** This histogram shows how engagement rates are distributed across all posts. The x-axis shows engagement level, y-axis shows count. Helps identify typical engagement patterns.")
        
        colors = ['#667eea', '#f093fb', '#4facfe']
        fig = go.Figure()
        for idx, col in enumerate(['post_length', 'previous_posts_count', 'follower_count']):
            fig.add_trace(go.Box(y=df[col], name=col, marker_color=colors[idx]))
        fig.update_layout(title='ğŸ“¦ Feature Distributions', font=dict(size=13, color='#1a1a1a'))
        st.plotly_chart(fig, use_container_width=True)
        st.info("ğŸ“¦ **What this shows:** Box plots compare the distribution of post length, previous posts count, and follower count. The box shows middle 50% of values, line shows median, dots show outliers.")
    
    with viz_tabs[1]:
        st.markdown("### ğŸ”— Correlation Matrix")
        
        numeric_df = df.select_dtypes(include=[np.number])
        corr_matrix = numeric_df.corr()
        
        fig = px.imshow(corr_matrix, text_auto='.2f', aspect="auto",
                       title='ğŸŒ¡ï¸ Feature Correlations',
                       color_continuous_scale='RdBu_r')
        fig.update_layout(font=dict(size=12, color='#1a1a1a'))
        st.plotly_chart(fig, use_container_width=True)
        st.info("ğŸ”¥ **What this shows:** This heatmap reveals relationships between all numeric features. Red = strong positive correlation, Blue = strong negative correlation, White = no correlation. Numbers show exact correlation strength (-1 to 1).")
    
    with viz_tabs[2]:
        st.markdown("### â° Time Analysis")
        
        time_counts = df.groupby(['time_of_day', 'risk_level']).size().reset_index(name='count')
        fig = px.bar(time_counts, x='time_of_day', y='count', color='risk_level',
                    title='â° Risk by Time of Day',
                    color_discrete_map={'Low': '#43e97b', 'Medium': '#f5af19', 'High': '#ff6b6b'})
        fig.update_layout(font=dict(size=13, color='#1a1a1a'))
        st.plotly_chart(fig, use_container_width=True)
    
    with viz_tabs[3]:
        st.markdown("### âš ï¸ Risk Analysis")
        
        risk_metrics = df.groupby('risk_level')[['engagement_rate', 'sentiment_score']].mean()
        
        fig = go.Figure()
        fig.add_trace(go.Bar(x=risk_metrics.index, y=risk_metrics['engagement_rate'],
                           name='Engagement', marker_color='#667eea'))
        fig.update_layout(title='ğŸ“Š Metrics by Risk Level', font=dict(size=13, color='#1a1a1a'))
        st.plotly_chart(fig, use_container_width=True)
    
    with viz_tabs[4]:
        st.markdown("### ğŸ“ Text Analysis")
        
        col1, col2 = st.columns(2)
        
        with col1:
            top_words = pd.Series(' '.join(df['text']).lower().split()).value_counts().head(20)
            fig = px.bar(x=top_words.values, y=top_words.index, orientation='h',
                        title='ğŸ“Š Top 20 Words', labels={'x': 'Count', 'y': 'Word'})
            fig.update_layout(font=dict(size=12, color='#1a1a1a'))
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            wordcloud_fig = create_wordcloud(df['text'])
            st.pyplot(wordcloud_fig)
            st.info("â˜ï¸ **What this shows:** Word cloud displays the most frequently used words in all posts. Larger words appear more often, helping identify common themes and concerns.")

elif page == "Visuals Pro" and st.session_state.data_loaded:
    st.markdown('<p class="main-header">ğŸ¨ VISUALIZATION STUDIO</p>', unsafe_allow_html=True)
    
    df = st.session_state.processed_data
    df = perform_text_analysis(df)
    
    viz_type = st.selectbox(
        "Select Visualization",
        ["3D Scatter", "Sunburst", "Parallel Coordinates", 
         "Radar Chart", "Treemap", "Sankey Diagram"]
    )
    
    if viz_type == "3D Scatter":
        st.info("ğŸ¯ **What this shows:** Three-dimensional view of post length, engagement rate, and follower count colored by risk level. Helps identify clustering patterns in high-risk content.")
        fig = px.scatter_3d(df, x='post_length', y='engagement_rate', z='follower_count',
                           color='risk_level', title='ğŸ¯ 3D Risk Scatter',
                           color_discrete_map={'Low': '#43e97b', 'Medium': '#f5af19', 'High': '#ff6b6b'})
        fig.update_layout(font=dict(size=12, color='#1a1a1a'))
        st.plotly_chart(fig, use_container_width=True)
    
    elif viz_type == "Sunburst":
        st.info("ğŸŒ… **What this shows:** Hierarchical sunburst showing risk distribution across time periods. Inner ring = time of day, outer rings = how risk levels distribute within each period.")
        fig = px.sunburst(df, path=['time_of_day', 'risk_level'],
                         title='ğŸŒ… Time-Risk Sunburst',
                         color='risk_level',
                         color_discrete_map={'Low': '#43e97b', 'Medium': '#f5af19', 'High': '#ff6b6b'})
        fig.update_layout(font=dict(size=12, color='#1a1a1a'))
        st.plotly_chart(fig, use_container_width=True)
    
    elif viz_type == "Parallel Coordinates":
        numeric_cols = ['post_length', 'engagement_rate', 'sentiment_score', 
                       'previous_posts_count', 'follower_count']
        df_viz = df.copy()
        risk_mapping = {'Low': 0, 'Medium': 1, 'High': 2}
        df_viz['risk_numeric'] = df_viz['risk_level'].map(risk_mapping)
        
        fig = px.parallel_coordinates(df_viz, dimensions=numeric_cols,
                                     color='risk_numeric',
                                     title='ğŸ¨ Parallel Coordinates',
                                     color_continuous_scale='RdYlGn_r',
                                     labels={'risk_numeric': 'Risk'})
        fig.update_layout(
            coloraxis_colorbar=dict(
                title="Risk",
                tickvals=[0, 1, 2],
                ticktext=['Low', 'Med', 'High']
            ),
            font=dict(size=12, color='#1a1a1a')
        )
        st.plotly_chart(fig, use_container_width=True)
        st.info("ğŸ“Š **What this shows:** Each line represents one post across multiple variables, colored by risk level. Crossing patterns reveal correlations - parallel lines mean features move together, crossing lines show inverse relationships.")
    
    elif viz_type == "Radar Chart":
        risk_stats = df.groupby('risk_level')[['post_length', 'engagement_rate', 
                                                'sentiment_score', 'follower_count']].mean()
        fig = go.Figure()
        colors = {'Low': '#43e97b', 'Medium': '#f5af19', 'High': '#ff6b6b'}
        for risk in risk_stats.index:
            fig.add_trace(go.Scatterpolar(
                r=risk_stats.loc[risk].values,
                theta=risk_stats.columns,
                fill='toself',
                name=risk,
                marker_color=colors[risk],
                opacity=0.6
            ))
        fig.update_layout(title='ğŸ¯ Risk Characteristics', font=dict(size=12, color='#1a1a1a'))
        st.plotly_chart(fig, use_container_width=True)
        st.info("ğŸ“¡ **What this shows:** Radar chart compares average feature values across risk categories. Each axis represents a different metric. Larger areas indicate higher values - helps visualize the profile of each risk level.")
    
    elif viz_type == "Treemap":
        st.info("ğŸ—ºï¸ **What this shows:** Treemap shows hierarchical data as nested rectangles. Size represents volume, color represents risk level, helping visualize proportions.")
        fig = px.treemap(df, path=['risk_level', 'time_of_day'],
                        title='ğŸ—ºï¸ Risk Treemap',
                        color='risk_level',
                        color_discrete_map={'Low': '#43e97b', 'Medium': '#f5af19', 'High': '#ff6b6b'})
        fig.update_traces(marker=dict(line=dict(width=2, color='white')))
        fig.update_layout(font=dict(size=12, color='#1a1a1a'))
        st.plotly_chart(fig, use_container_width=True)
    
    elif viz_type == "Sankey Diagram":
        st.info("ğŸŒŠ **What this shows:** Flow diagram showing how posts progress from time of day â†’ day of week â†’ risk level. Ribbon width indicates volume, revealing when high-risk content appears.")
        time_day_risk = df.groupby(['time_of_day', 'day_of_week', 'risk_level']).size().reset_index(name='count')
        
        all_times = df['time_of_day'].unique()
        all_days = df['day_of_week'].unique()
        all_risks = df['risk_level'].unique()
        
        labels = list(all_times) + list(all_days) + list(all_risks)
        
        time_to_idx = {t: i for i, t in enumerate(all_times)}
        day_to_idx = {d: i + len(all_times) for i, d in enumerate(all_days)}
        risk_to_idx = {r: i + len(all_times) + len(all_days) for i, r in enumerate(all_risks)}
        
        sources = []
        targets = []
        values = []
        
        time_day = df.groupby(['time_of_day', 'day_of_week']).size().reset_index(name='count')
        for _, row in time_day.iterrows():
            sources.append(time_to_idx[row['time_of_day']])
            targets.append(day_to_idx[row['day_of_week']])
            values.append(row['count'])
        
        day_risk = df.groupby(['day_of_week', 'risk_level']).size().reset_index(name='count')
        for _, row in day_risk.iterrows():
            sources.append(day_to_idx[row['day_of_week']])
            targets.append(risk_to_idx[row['risk_level']])
            values.append(row['count'])
        
        colors_list = ['rgba(102, 126, 234, 0.4)'] * len(all_times) + \
                     ['rgba(240, 147, 251, 0.4)'] * len(all_days) + \
                     ['rgba(67, 233, 123, 0.8)', 'rgba(245, 175, 25, 0.8)', 'rgba(255, 107, 107, 0.8)']
        
        fig = go.Figure(data=[go.Sankey(
            node=dict(
                pad=15,
                thickness=20,
                line=dict(color='white', width=2),
                label=labels,
                color=colors_list
            ),
            link=dict(
                source=sources,
                target=targets,
                value=values,
                color='rgba(200, 200, 200, 0.3)'
            )
        )])
        
        fig.update_layout(
            title='ğŸŒŠ Time â†’ Day â†’ Risk Flow',
            font=dict(size=14, color='#1a1a1a'),
            height=600
        )
        st.plotly_chart(fig, use_container_width=True)

elif page == "Engineer" and st.session_state.data_loaded:
    st.markdown('<p class="main-header">âš™ï¸ FEATURE ENGINEERING</p>', unsafe_allow_html=True)
    
    df = st.session_state.processed_data
    
    st.markdown("### ğŸ› ï¸ Create New Features")
    
    feature_type = st.selectbox("Feature Type", 
                               ["Text Length", "Engagement Category", "Time Features", "Sentiment Category"])
    
    if feature_type == "Text Length":
        if st.button("Create Feature"):
            df['text_length_category'] = pd.cut(df['post_length'], 
                                               bins=[0, 50, 100, 200],
                                               labels=['Short', 'Medium', 'Long'])
            st.session_state.processed_data = df
            st.success("âœ… Text length categories created!")
    
    elif feature_type == "Engagement Category":
        if st.button("Create Feature"):
            df['engagement_category'] = pd.cut(df['engagement_rate'],
                                              bins=[0, 0.3, 0.7, 1.0],
                                              labels=['Low', 'Medium', 'High'])
            st.session_state.processed_data = df
            st.success("âœ… Engagement categories created!")

elif page == "AI Models" and st.session_state.data_loaded:
    st.markdown('<p class="main-header">ğŸ¤– AI MODEL TRAINING</p>', unsafe_allow_html=True)
    
    df = st.session_state.processed_data
    
    model_tabs = st.tabs(["ğŸ¯ Train", "ğŸ“Š Results", "ğŸ›ï¸ Tune"])
    
    with model_tabs[0]:
        st.markdown("### ğŸ¯ Model Training")
        
        selected_models = st.multiselect(
            "Select Models",
            ["Random Forest", "Gradient Boosting", "Logistic Regression"],
            default=["Random Forest"]
        )
        
        if st.button("ğŸš€ START TRAINING", use_container_width=True, type="primary"):
            with st.spinner("Training models..."):
                numeric_features = ['post_length', 'engagement_rate', 'sentiment_score', 
                                  'previous_posts_count', 'follower_count']
                
                X = df[numeric_features]
                y = df['risk_level']
                
                le = LabelEncoder()
                y_encoded = le.fit_transform(y)
                
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y_encoded, test_size=0.2, random_state=42
                )
                
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                
                models = {}
                
                if "Random Forest" in selected_models:
                    rf = RandomForestClassifier(n_estimators=100, random_state=42)
                    rf.fit(X_train_scaled, y_train)
                    rf_pred = rf.predict(X_test_scaled)
                    models['Random Forest'] = {
                        'model': rf,
                        'predictions': rf_pred,
                        'accuracy': (rf_pred == y_test).mean()
                    }
                
                if "Gradient Boosting" in selected_models:
                    gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
                    gb.fit(X_train_scaled, y_train)
                    gb_pred = gb.predict(X_test_scaled)
                    models['Gradient Boosting'] = {
                        'model': gb,
                        'predictions': gb_pred,
                        'accuracy': (gb_pred == y_test).mean()
                    }
                
                if "Logistic Regression" in selected_models:
                    lr = LogisticRegression(random_state=42, max_iter=1000)
                    lr.fit(X_train_scaled, y_train)
                    lr_pred = lr.predict(X_test_scaled)
                    models['Logistic Regression'] = {
                        'model': lr,
                        'predictions': lr_pred,
                        'accuracy': (lr_pred == y_test).mean()
                    }
                
                st.session_state.models = models
                st.session_state.X_test = X_test_scaled
                st.session_state.y_test = y_test
                st.session_state.label_encoder = le
                st.session_state.X_train_scaled = X_train_scaled
                st.session_state.y_train = y_train
                
                performance_df = pd.DataFrame({
                    'Model': list(models.keys()),
                    'Accuracy': [m['accuracy'] for m in models.values()]
                }).sort_values('Accuracy', ascending=False)
                
                st.dataframe(performance_df, use_container_width=True)
                st.info("ğŸ¯ **What this table shows:** AI model accuracy comparison. Higher scores mean better predictions. Use this to select the best model.")
                
                st.success("âœ… Training complete!")
    
    with model_tabs[1]:
        if 'models' in st.session_state:
            st.markdown("### ğŸ“Š Performance")
            
            # Get y_test and le from session state
            y_test = st.session_state.y_test
            le = st.session_state.label_encoder
            
            for model_name, result in st.session_state.models.items():
                with st.expander(f"ğŸ¯ {model_name} - {result['accuracy']:.4f}", expanded=True):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        report = classification_report(y_test, result['predictions'], 
                                                     target_names=le.classes_,
                                                     output_dict=True)
                        st.dataframe(pd.DataFrame(report).T, use_container_width=True)
                        st.info("ğŸ“Š **What this table shows:** Performance metrics per risk level. Precision = accuracy of predictions (how many predicted High were actually High), Recall = coverage (how many actual High were caught), F1-Score = balanced measure combining both.")
                    
                    with col2:
                        cm = confusion_matrix(y_test, result['predictions'])
                        fig = px.imshow(cm, text_auto=True, aspect="auto",
                                       x=le.classes_, y=le.classes_,
                                       color_continuous_scale='Blues')
                        fig.update_layout(title=f"{model_name}", font=dict(color='#1a1a1a'))
                        st.plotly_chart(fig, use_container_width=True)
                        st.info("ğŸ¯ **What this shows:** Confusion matrix displays actual vs predicted classifications. Diagonal values (dark blue) are correct predictions. Off-diagonal values show misclassifications - helps identify where the model makes mistakes.")
        else:
            st.warning("Train models first!")
    
    with model_tabs[2]:
        st.markdown("### ğŸ›ï¸ Tuning")
        
        if 'models' in st.session_state:
            # Get training data from session state
            X_train_scaled = st.session_state.get('X_train_scaled')
            y_train = st.session_state.get('y_train')
            
            if X_train_scaled is not None and y_train is not None:
                model_choice = st.selectbox("Model", ["Random Forest", "Gradient Boosting"])
                
                if model_choice == "Random Forest":
                    n_est = st.slider("Trees", 50, 300, 100, 25)
                    max_d = st.slider("Depth", 5, 50, 10, 5)
                    
                    if st.button("Optimize"):
                        with st.spinner("Tuning..."):
                            model = RandomForestClassifier(n_estimators=n_est, max_depth=max_d, random_state=42)
                            scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
                            st.success(f"Score: {scores.mean():.4f} Â± {scores.std():.4f}")
                
                else:
                    lr = st.slider("Learning Rate", 0.01, 0.3, 0.1, 0.01)
                    n_est = st.slider("Estimators", 50, 300, 100, 25)
                    
                    if st.button("Optimize"):
                        with st.spinner("Tuning..."):
                            model = GradientBoostingClassifier(learning_rate=lr, n_estimators=n_est, random_state=42)
                            scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
                            st.success(f"Score: {scores.mean():.4f} Â± {scores.std():.4f}")
            else:
                st.warning("Train models first in the Train tab!")
        else:
            st.warning("Train models first in the Train tab!")

elif page == "Predict" and st.session_state.data_loaded:
    st.markdown('<p class="main-header">ğŸ”® AI PREDICTION ENGINE</p>', unsafe_allow_html=True)
    
    st.markdown("### ğŸ’¬ Enter Post Content")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        text_input = st.text_area("Post Content", 
                                   "Type or paste the social media post here...", 
                                   height=200,
                                   help="Enter any text - the AI will use transformer-based semantic understanding")
        
        st.markdown("""
        **ğŸ’¡ Try these examples:**
        - "I am skeptical whether to live or not. I hate this life." (Should be Medium-High)
        - "I don't want to live" (Should be High)
        - "The weather is good. I want to travel." (Should be Low - unrelated)
        - "I'm feeling happy today" (Should be Low)
        """)
    
    with col2:
        st.markdown("### âš™ï¸ About the Model")
        st.info("""
        ğŸ§  **Transformer-Based**
        - Uses Sentence-BERT embeddings
        - Understands semantic meaning
        - Not keyword-dependent
        - Context-aware classification
        """)

    if st.button("ğŸ¯ ANALYZE RISK", use_container_width=True, type="primary"):
        if len(text_input.strip()) < 5:
            st.warning("âš ï¸ Please enter at least 5 characters")
        else:
            with st.spinner("ğŸ¤– AI analyzing with transformers..."):
                progress = st.progress(0)
                for i in range(100):
                    time.sleep(0.01)
                    progress.progress(i + 1)
                
                result = analyze_mental_health_risk_transformer(text_input)
                progress.empty()
                
                risk_level = result['risk_level']
                risk_score = result['risk_score']
                confidence = result['confidence']
                proba = result['probabilities']
                detected_factors = result['detected_factors']
                is_safe_unrelated = result.get('is_safe_unrelated', False)
                
                # Special handling for safe unrelated content
                if is_safe_unrelated:
                    st.success("âœ… **NO SUICIDE INTENT DETECTED** - This message does not contain any suicide-related content or mental health crisis indicators. The content appears safe and unrelated to self-harm.")
                
                if risk_level == "High":
                    color, icon, message = "#ff6b6b", "ğŸ”´", "HIGH RISK - IMMEDIATE ATTENTION"
                elif risk_level == "Medium":
                    color, icon, message = "#f5af19", "ğŸŸ¡", "MEDIUM RISK - CLOSE MONITORING"
                else:
                    color, icon, message = "#43e97b", "ğŸŸ¢", "LOW RISK - STANDARD MONITORING"
                
                st.markdown("---")
                
                if detected_factors:
                    st.markdown("### ğŸ” Analysis:")
                    for factor in detected_factors[:8]:
                        if "ğŸ”´" in factor:
                            st.error(factor)
                        elif "ğŸŸ¡" in factor:
                            st.warning(factor)
                        else:
                            st.success(factor)
                
                st.markdown("<br>", unsafe_allow_html=True)
                
                st.markdown(f"""
                <div style='background: {color}; padding: 50px; border-radius: 20px; color: white; 
                            text-align: center; box-shadow: 0 10px 30px {color}60; 
                            border: 3px solid rgba(255,255,255,0.3);'>
                    <div style='font-size: 5rem;'>{icon}</div>
                    <h1 style='font-size: 3rem; margin: 20px 0; color: white !important; font-weight: 900;'>{risk_level.upper()} RISK</h1>
                    <p style='font-size: 1.3rem; color: white !important; font-weight: 600;'>{message}</p>
                    <h2 style='font-size: 2.5rem; margin-top: 30px; color: white !important;'>Score: {risk_score}/100</h2>
                    <p style='font-size: 1.2rem; color: white !important;'>{confidence*100:.1f}% Confidence</p>
                </div>
                """, unsafe_allow_html=True)
                
                st.markdown("<br>", unsafe_allow_html=True)
                
                col1, col2, col3 = st.columns(3)
                col1.metric("ğŸŸ¢ Low", f"{proba[0]*100:.1f}%")
                col2.metric("ğŸŸ¡ Medium", f"{proba[1]*100:.1f}%")
                col3.metric("ğŸ”´ High", f"{proba[2]*100:.1f}%")
                
                with st.expander("ğŸ“Š Technical Details", expanded=False):
                    col1, col2, col3, col4 = st.columns(4)
                    col1.metric("Risk Score", f"{risk_score}/100")
                    col2.metric("Sentiment", f"{result['sentiment']:.2f}")
                    if 'high_similarity' in result:
                        col3.metric("High Sim", f"{result['high_similarity']:.2f}")
                        col4.metric("Med Sim", f"{result['medium_similarity']:.2f}")
                    
                    st.markdown("**All Factors:**")
                    for factor in detected_factors:
                        st.write(factor)
                
                st.markdown("---")
                st.markdown("### ğŸ’¡ Recommendations")
                
                if risk_level == "High":
                    st.markdown("""
                    <div style='background: linear-gradient(135deg, #ff6b6b, #f5576c); padding: 35px; 
                                border-radius: 15px; color: white; box-shadow: 0 8px 25px rgba(255,107,107,0.4);'>
                        <h3 style='color: white !important;'>ğŸš¨ IMMEDIATE ACTION</h3>
                        <ul style='font-size: 1.1rem; line-height: 2.2; color: white !important;'>
                            <li><b>Call 988</b> (USA Suicide Lifeline)</li>
                            <li><b>Text HOME to 741741</b></li>
                            <li><b>Go to emergency room</b></li>
                            <li><b>Do not leave alone</b></li>
                            <li><b>Contact mental health professional</b></li>
                        </ul>
                    </div>
                    """, unsafe_allow_html=True)
                elif risk_level == "Medium":
                    st.markdown("""
                    <div style='background: linear-gradient(135deg, #f5af19, #f39c12); padding: 30px; 
                                border-radius: 15px; color: white; box-shadow: 0 8px 25px rgba(245,175,25,0.4);'>
                        <h3 style='color: white !important;'>âš ï¸ CLOSE MONITORING</h3>
                        <ul style='font-size: 1.05rem; line-height: 2; color: white !important;'>
                            <li><b>Check in regularly</b></li>
                            <li><b>Encourage professional help</b></li>
                            <li><b>Monitor for escalation</b></li>
                            <li><b>Provide crisis resources</b></li>
                        </ul>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    st.markdown("""
                    <div style='background: linear-gradient(135deg, #43e97b, #38f9d7); padding: 25px; 
                                border-radius: 15px; color: white; box-shadow: 0 8px 25px rgba(67,233,123,0.4);'>
                        <h3 style='color: white !important;'>âœ… STANDARD MONITORING</h3>
                        <ul style='font-size: 1rem; line-height: 2; color: white !important;'>
                            <li><b>Continue regular check-ins</b></li>
                            <li><b>Encourage positive activities</b></li>
                            <li><b>Maintain awareness</b></li>
                        </ul>
                    </div>
                    """, unsafe_allow_html=True)

elif page == "Docs":
    st.markdown('<p class="main-header">ğŸ“š DOCUMENTATION</p>', unsafe_allow_html=True)
    
    doc_tabs = st.tabs(["Overview", "Features", "Methods", "Usage"])
    
    with doc_tabs[0]:
        st.markdown("""
        ## ğŸ¯ Project Overview
        
        **MindGuard Analytics** is an AI-powered mental health risk assessment system using:
        - ğŸ§  **Transformer-based NLP** (Sentence-BERT)
        - ğŸ“Š **Semantic understanding** (not keyword matching)
        - ğŸ¯ **Multi-class classification** (Low/Medium/High)
        - ğŸ’¡ **Context-aware** analysis
        
        ### ğŸŒŸ Key Innovation
        Unlike traditional keyword-matching systems, MindGuard uses **sentence embeddings** to understand
        the semantic meaning of text, allowing it to:
        - Detect suicidal ideation even without explicit keywords
        - Understand context and nuance
        - Generalize to diverse language patterns
        - Reduce false negatives
        """)
    
    with doc_tabs[1]:
        st.markdown("""
        ## âœ¨ Features
        
        ### ğŸ”® Prediction Engine
        - **Transformer-based**: Uses Sentence-BERT for semantic understanding
        - **Semantic similarity**: Compares input to reference examples
        - **Multi-class**: Low, Medium, High risk classification
        - **Confidence scores**: Probability distribution across classes
        
        ### ğŸ“Š Visualization
        - 13+ interactive charts with explanations
        - Correlation analysis
        - Time-series patterns
        - Risk distribution
        
        ### ğŸ¤– Machine Learning
        - Random Forest, Gradient Boosting, Logistic Regression
        - Model comparison and tuning
        - Performance metrics
        - Confusion matrices
        """)
    
    with doc_tabs[2]:
        st.markdown("""
        ## ğŸ§  Methodology
        
        ### Transformer-Based Analysis
        
        **1. Sentence Embedding**
        - Uses Sentence-BERT (all-MiniLM-L6-v2)
        - Converts text to 384-dimensional vectors
        - Captures semantic meaning
        
        **2. Reference Examples**
        - High risk: multiple examples of suicidal ideation
        - Medium risk: diverse distress statements
        - Low risk: positive/neutral examples
        
        **3. Cosine Similarity**
        - Compares input embedding to each reference category
        - Calculates similarity scores (0-1)
        - Combines with sentiment analysis
        
        **4. Classification**
        - Uses similarity scores + sentiment
        - Determines risk level (High/Medium/Low)
        - Generates confidence and probabilities
        """)
    
    with doc_tabs[3]:
        st.markdown("""
        ## ğŸ“– Usage Guide
        
        ### 1. Load Data
        Click **"LOAD DEMO DATA"** on the Home page
        
        ### 2. Explore Data
        - **Data Hub**: View and clean data
        - **Explorer**: Analyze distributions
        - **Visuals Pro**: Advanced visualizations
        
        ### 3. Train Models (Optional)
        - Go to **AI Models** page
        - Select models to train
        - View performance metrics
        
        ### 4. Make Predictions
        - Go to **Predict** page
        - Enter text (any language about mental health)
        - Click **ANALYZE RISK**
        - View results with semantic similarity scores
        
        ### Example Inputs
        - âœ… "I am skeptical whether to live or not. I hate this life." â†’ Medium-High
        - âœ… "I don't want to live" â†’ High
        - âœ… "The weather is good" â†’ Low (unrelated)
        - âœ… "I feel empty and worthless" â†’ High
        """)
# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; padding: 20px; color: #1a1a1a;'>
    <p>ğŸ›¡ï¸ <b>MindGuard Analytics</b> | Transformer-Based Mental Health Risk Assessment</p>
    <p style='font-size: 0.9rem;'>Using Sentence-BERT for Semantic Understanding | Not keyword-dependent</p>
</div>
""", unsafe_allow_html=True)
